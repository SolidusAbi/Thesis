{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import os, sys\n",
    "project_root_dir = os.path.join(os.getcwd(),'../..')\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.append(project_root_dir)\n",
    "    import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureSelection.Experiments.BrainCancerHSIBandSelection import BrainCancerHSIBandSelectionBase\n",
    "\n",
    "test = BrainCancerHSIBandSelectionBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset = 2000\n",
    "batch_size = max(len_dataset // 100, 32)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentBase:\n",
    "    def config(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "from FeatureSelection.Experiments import Model\n",
    "class BrainCancerHSIBandSelectionBase(ExperimentBase):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = Model([128, 150, 64, 3], fs_threshold=.9, fs_tau=.5) \n",
    "\n",
    "    def config(self):\n",
    "        return {\n",
    "            'dataset_dir': config.BRAIN_HSI_DIR,\n",
    "            'dataset': 'BrainCancerHSI',\n",
    "            'model': self.model,\n",
    "            'train_size': 0.8,\n",
    "            'test_size': 0.2,\n",
    "            'batch_size': 32,\n",
    "            # 'batch_size': 64,\n",
    "            # 'batch_size': 128,\n",
    "            'epochs': 200,\n",
    "            'lr': 1e-3,\n",
    "            'seed': 42,\n",
    "            'log_interval': 10,\n",
    "            'save_model': False,\n",
    "            'save_model_dir': 'Test', #config.MODEL_DIR,\n",
    "            'save_model_name': 'BrainCancerHSIBandSelection.pt',\n",
    "            'save_result': True,\n",
    "            'save_result_dir': os.path.join(config.RESULTS_DIR, 'Chapter6/BrainCancerHSI'),\n",
    "            'save_result_name': 'BrainCancerHSIBandSelection.pkl'\n",
    "        }\n",
    "\n",
    "from FeatureSelection.Experiments import BrainCancerHSIDataset\n",
    "from FeatureSelection.Experiments.utils import Normalize\n",
    "import pandas as pd\n",
    "\n",
    "class VNIRimagesOp8C1(BrainCancerHSIBandSelectionBase):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.experiment = 'VNIRimagesOp8C1'\n",
    "        self.data_dir = os.path.join(config.BRAIN_HSI_DIR, f'preprocessed/data/no_outliers/{self.experiment}/')\n",
    "\n",
    "        mean_std_df = pd.read_csv(os.path.join(self.data_dir, 'mean_std.csv'))\n",
    "        self.transform = Normalize(mean_std_df['mean'], mean_std_df['std'])\n",
    "        self.dataset = BrainCancerHSIDataset(self.data_dir, transform=self.transform)\n",
    "        \n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        test_size = len(self.dataset) - train_size\n",
    "        self.train_dataset, self.test_dataset = torch.utils.data.random_split(self.dataset, [train_size, test_size], \n",
    "                                generator=generator)\n",
    "\n",
    "    def config(self):\n",
    "        base_config = super().config()\n",
    "\n",
    "        experiment = 'VNIRimagesOp8C1'\n",
    "        config = {\n",
    "            'experiment_name': experiment,\n",
    "            'data_dir': self.data_dir,\n",
    "            'train_dataset': self.train_dataset,\n",
    "            'test_dataset': self.test_dataset,\n",
    "            'save_model_name': '{}.pt'.format(experiment),\n",
    "            'reg_factor': 2,\n",
    "            'weighted_sampler': True,\n",
    "        }\n",
    "\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def run(self):\n",
    "        return super().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureSelection.Experiments import BrainCancerHSIDataset\n",
    "from FeatureSelection.Experiments.utils import Normalize\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = '/home/abian/Data/Dataset/IUMA/Experimento (Abian)/preprocessed/data/no_outliers/VNIRimagesOp8C1'\n",
    "mean_std_df = pd.read_csv(os.path.join(data_dir, 'mean_std.csv'))\n",
    "transform = Normalize(mean_std_df['mean'], mean_std_df['std'])\n",
    "dataset = BrainCancerHSIDataset(data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size], \n",
    "                        generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y = train_dataset[:]\n",
    "\n",
    "torch.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y = test_dataset[:]\n",
    "\n",
    "torch.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "def cosine_scheduler(timesteps, s=8e-3):\n",
    "    r'''\n",
    "        Cosine scheduler for the regularization factor.\n",
    "\n",
    "        Reference:\n",
    "            Nichol, A. Q., & Dhariwal, P. (2021, July). Improved denoising\n",
    "            diffusion probabilistic models. In International Conference on Machine \n",
    "            Learning (pp. 8162-8171). PMLR.\n",
    "    '''\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x/timesteps)+s) / (1+s) * math.pi * .5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    return 1 - alphas_cumprod\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def _weights(dataset):\n",
    "    '''\n",
    "        Compute the weights for each sample in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            weights: np.array\n",
    "                The weights for each sample in the dataset.\n",
    "    '''\n",
    "    _, y = dataset[:]\n",
    "    count = torch.bincount(y)\n",
    "    # count = list(map(lambda i: (y==i).sum(), range(3)))\n",
    "    weights = 1. / np.array(count)\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    return weights[y]\n",
    "\n",
    "def log(tb_writer, model, loss, epoch):\n",
    "    tb_writer.add_scalar('Loss/test', loss, epoch)\n",
    "    tb_writer.add_scalar('Sparse Rate', model.sparse_rate(), epoch)\n",
    "    phi = model.feature_selector.variational_parameter()\n",
    "    for i in range(phi.shape[0]):\n",
    "        tb_writer.add_scalar(f'Phi/{i}', phi[i], epoch)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "def train(model, train_dataset, test_dataset, tb_writer=None, **kwargs) :\n",
    "\n",
    "    log_interval = kwargs['log_interval'] if 'log_interval' in kwargs else 10\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    def seed_worker(worker_id):\n",
    "        '''\n",
    "            Seed the workers for reproducibility.\n",
    "\n",
    "            Reference:\n",
    "            ----------\n",
    "                https://pytorch.org/docs/stable/notes/randomness.html\n",
    "        '''\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        np.seed(worker_seed)\n",
    "\n",
    "    seed = kwargs['seed'] if 'seed' in kwargs else 42\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "\n",
    "    lr = kwargs['lr'] if 'lr' in kwargs else 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    batch_size = kwargs['batch_size'] if 'batch_size' in kwargs else 32\n",
    "\n",
    "    weighted_sampler = kwargs['weighted_sampler'] if 'weighted_sampler' in kwargs else False\n",
    "    if weighted_sampler:\n",
    "        sampler = WeightedRandomSampler(_weights(train_dataset), len(train_dataset), replacement=True)\n",
    "    else:\n",
    "        # Use the RandomSampler for the training dataset\n",
    "        sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, worker_init_fn=seed_worker, generator=generator)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size*4, worker_init_fn=seed_worker, shuffle=False)\n",
    "\n",
    "    n_epochs = kwargs['epochs'] if 'epochs' in kwargs else 10\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1, 'sparse_rate': \"%.2f\" % 0},\n",
    "        )\n",
    "      \n",
    "  \n",
    "    reg = kwargs['reg_factor'] if 'reg_factor' in kwargs else 1\n",
    "    reg_factor = cosine_scheduler(n_epochs, 1e-3) * reg\n",
    "    print(f'n_epochs: {n_epochs}')\n",
    "    print(f'batch_size: {batch_size}')\n",
    "    print(f'lr: {lr}')\n",
    "    print(f'weighted_sampler: {weighted_sampler}')\n",
    "    print(f'log_interval: {log_interval}')\n",
    "    print(f'seed: {seed}')\n",
    "    print(f'reg_factor: {reg}')\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        model.train()\n",
    "        for idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            loss = criterion(pred, targets) + model.regularization(reg_factor[epoch].item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                pred = model(inputs)\n",
    "                loss = criterion(pred, targets)\n",
    "                test_loss += loss.item()\n",
    "            \n",
    "            test_loss /= len(test_loader)\n",
    "            epoch_iterator.set_postfix(reg=\"%.3f\" % reg_factor[epoch], tls=\"%.4f\" % test_loss, sparse_rate=\"%.2f\" % model.sparse_rate())\n",
    "\n",
    "            if epoch % log_interval == 0 and tb_writer is not None:\n",
    "                log(tb_writer, model, test_loss, epoch) \n",
    "\n",
    "\n",
    "    return model.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std) -> None:\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.tensor(mean).float()\n",
    "        self.std = torch.tensor(std).float()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        std_inv = 1 / (self.std + 1e-16)\n",
    "        mean_inv = -self.means * std_inv\n",
    "\n",
    "        return (x - mean_inv) / std_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = VNIRimagesOp8C1()\n",
    "exp.config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureSelection.Experiments import BrainCancerHSIDataset\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('/home/abian/Data/Dataset/IUMA/Experimento (Abian)/preprocessed/data/no_outliers/VNIRimagesOp8C1/mean_std.csv')\n",
    "# transform = Normalize(df['mean'], df['std'])\n",
    "# dataset = BrainCancerHSIDataset('/home/abian/Data/Dataset/IUMA/Experimento (Abian)/preprocessed/data/no_outliers/VNIRimagesOp8C1/', transform=transform)\n",
    "# model = Model([128, 256, 64, 3])\n",
    "\n",
    "exp = VNIRimagesOp8C1()\n",
    "# model = train(model, dataset, dataset, batch_size=128, weighted_sampler=True, reg_factor=1.5, epochs=300)\n",
    "model = train( **exp.config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(torch.sigmoid(model.cpu().feature_selector.variational_parameter()).detach().numpy(), bins=10)\n",
    "\n",
    "# plt.hist(F.hardtanh(model.cpu().feature_selector.variational_parameter().detach(), 0, 1).numpy(), bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((torch.sigmoid(model.feature_selector.variational_parameter()).detach().numpy() < 0.5).sum())\n",
    "print((torch.sigmoid(model.feature_selector.variational_parameter()).detach().numpy() < 0.5))\n",
    "\n",
    "# Gaussian\n",
    "# print(F.hardtanh(model.cpu().feature_selector.variational_parameter().detach(), 0, 1).numpy().sum())\n",
    "# print((F.hardtanh(model.cpu().feature_selector.variational_parameter().detach(), 0, 1).numpy() > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_band_selection(features_selected:np.ndarray, samples:list, sample_labels=None, wavelength=None, n_features=-1):\n",
    "    if sample_labels is not None:\n",
    "        assert(len(samples) == len(sample_labels))\n",
    "    else:\n",
    "        sample_labels = [None] * len(samples)\n",
    "\n",
    "    diff = np.diff(features_selected)\n",
    "    features_ranges = np.split(features_selected, np.where(diff != 1)[0]+1)\n",
    "\n",
    "    with plt.style.context('seaborn-colorblind'):\n",
    "        fig = plt.figure()\n",
    "        for idx, sample in enumerate(samples):\n",
    "            plt.plot(sample, label=sample_labels[idx])\n",
    "            plt.scatter(features_selected, sample[features_selected], alpha=.5)\n",
    "\n",
    "        for r in features_ranges:\n",
    "            plt.axvspan(r[0]-.25, r[-1]+.25, alpha=0.25)\n",
    "\n",
    "        if wavelength is not None:\n",
    "            ticks = np.linspace(0, len(wavelength)-1, 12, dtype=int)\n",
    "            plt.xlabel('Wavelength (nm)', fontsize='x-large')\n",
    "            plt.xticks(ticks, wavelength[ticks], rotation=45, fontsize='large')\n",
    "\n",
    "        plt.ylabel('Reflectance', fontsize='x-large')\n",
    "        plt.yticks(fontsize='large')\n",
    "        \n",
    "        if sample_labels[0] is not None:\n",
    "            plt.legend(loc='upper right')\n",
    "    \n",
    "        if n_features != -1:\n",
    "            sparse_rate = (n_features - len(features_selected)) / n_features \n",
    "            title = 'Sparsity: {:.2f}'.format(sparse_rate)\n",
    "            plt.title(title, fontsize='xx-large')\n",
    " \n",
    "        plt.margins(x=0.01)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# mat = loadmat('/home/abian/Data/Dataset/IUMA/Experimento (Abian)/preProcessedWavelength.mat')\n",
    "# wv = mat['preProcessedVnirWl']\n",
    "\n",
    "X, y = dataset.X, dataset.y\n",
    "print(y[1000], y[100], y[3500])\n",
    "\n",
    "# Concrete\n",
    "fig = plot_band_selection(torch.where(torch.sigmoid(model.feature_selector.variational_parameter()) < 0.9)[0], X[[1000, 100, 3500]], sample_labels=['Healthy', 'Tumor', 'Vessel'], n_features=128)\n",
    "\n",
    "# Gaussian\n",
    "# fig = plot_band_selection(torch.where(F.hardtanh(model.cpu().feature_selector.variational_parameter().detach(), 0, 1) > 0)[0], X[[1000, 100, 3500]], sample_labels=['Healthy', 'Tumor', 'Vessel'], n_features=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.clip(config.WAVELENGTH, config.clip_wavelenght_range[0], config.clip_wavelenght_range[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = VNIRimagesOp8C1()\n",
    "\n",
    "a.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp8C2.csv'))\n",
    "y_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp8C2_gt.csv'))\n",
    "\n",
    "X_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp12C2.csv'))\n",
    "y_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp12C2_gt.csv'))\n",
    "\n",
    "X_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp15C1.csv'))\n",
    "y_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp15C1_gt.csv'))\n",
    "\n",
    "X_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp20C1.csv'))\n",
    "y_df = pd.read_csv(os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/', 'VNIRimagesOp20C1_gt.csv'))\n",
    "\n",
    "X = X_df.to_numpy()\n",
    "y = y_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in np.unique(y):\n",
    "    print(i, np.sum(y == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(config.BRAIN_HSI_DIR, 'preprocessed/data/no_outliers/VNIRimagesOp8C1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [128, 254, 64, 32, 3]\n",
    "\n",
    "for idx, (x, y) in enumerate(sliding_window_iter(hidden_size, 2)):\n",
    "    print(x,y)\n",
    "    # check if it is the iteration\n",
    "    print(idx)\n",
    "    if idx == ((len(hidden_size) + 1) // 2):\n",
    "        print('Ultima')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hidden_layer(in_features, out_features, output=False):\n",
    "    if output:\n",
    "        return nn.Linear(in_features, out_features)\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features, affine=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5),\n",
    "        )\n",
    "\n",
    "layers = []\n",
    "for idx, (in_features, out_features) in enumerate(sliding_window_iter(hidden_size, 2)):\n",
    "    last = (True if idx == ((len(hidden_size) + 1) // 2) else False)\n",
    "    layers.append(_hidden_layer(in_features, out_features, last))\n",
    "\n",
    "nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [128, 254, 64, 32, 3]\n",
    "print((len(hidden_size )+1)//2)\n",
    "\n",
    "hidden_size = [128, 254, 64, 3]\n",
    "print((len(hidden_size)+1)//2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot gaussian PDF\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.functional import hardsigmoid\n",
    "\n",
    "x = np.linspace(-1.5, 2.5, 1000)\n",
    "ud = 0.6\n",
    "epsilon_gauss = norm.pdf(x, ud, 0.5)\n",
    "\n",
    "hard_tanh_range = (0, 1)\n",
    "hard = np.clip(x, *hard_tanh_range)\n",
    "\n",
    "# concrete\n",
    "eps = 1e-8\n",
    "# rho = np.arange(0, 1, 0.01)\n",
    "# unif_noise = 0.6\n",
    "\n",
    "rho = 0.2\n",
    "epsilon_unif = np.arange(0, 1, 0.01)\n",
    "s = torch.tensor(np.log(rho/(1-rho) + eps) + np.log(epsilon_unif + eps) - np.log((1-epsilon_unif) + eps))\n",
    "concrete = torch.sigmoid(1/5e-1 * s)\n",
    "# concrete = hardsigmoid(1/3e-1 * s)\n",
    "\n",
    "# hard-concrete\n",
    "gamma = -0.1\n",
    "zeta = 1.1\n",
    "s_hat = (concrete*(zeta-gamma)) + (gamma)\n",
    "hard_concrete = torch.nn.functional.hardtanh(s_hat, 0, 1)\n",
    "\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "\n",
    "    # Concrete Relaxation\n",
    "    ## Concrete\n",
    "    ax[0].plot(epsilon_unif, concrete, label=f'$f_C(\\\\phi_i, \\\\epsilon_i)$')\n",
    "    ax[0].set_ylabel(f'$S$', fontsize='x-large')\n",
    "    # ax[0].set_xlabel(f'$\\\\phi_i$', fontsize='x-large')\n",
    "\n",
    "    ## Epsilon\n",
    "    # ax[1].plot(rho, np.ones_like(rho), c='r', alpha=0.5, label=f'$\\\\epsilon_i = \\\\mathbb{{U}}(0,1)$')\n",
    "\n",
    "    ## Hard-Concrete\n",
    "    ax[0].plot(epsilon_unif, hard_concrete, label=f'$f_{{HC}}(\\\\phi_i, \\\\epsilon_i)$')\n",
    "\n",
    "    # phi_i\n",
    "    ax[0].axvline(1-rho, c='r', linestyle='--', label='$\\\\phi_i = {}$'.format(1-rho))\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].set_title('Concrete and Hard-Concrete Relaxation')\n",
    "\n",
    "    # Gaussian Relaxation\n",
    "    ## Hard Tanh\n",
    "    ax[1].plot(x, hard, label=f'$f_G(\\\\phi_i, \\\\epsilon_i)$')\n",
    "    ax[1].axvline(hard_tanh_range[0], c='k', linestyle='--')\n",
    "    ax[1].axvline(hard_tanh_range[1], c='k', linestyle='--')\n",
    "    ax[1].set_ylabel(f'$S$', fontsize='x-large')\n",
    "    ax[1].set_xticks([hard_tanh_range[0], hard_tanh_range[1]])\n",
    "    # ax[1].set_xlabel(f'$\\\\mu_i$', fontsize = 'x-large')\n",
    "\n",
    "    ## Epsilon\n",
    "    ax[1].plot(x, epsilon_gauss, c='r')\n",
    "    ax[1].plot(x, epsilon_gauss, label = f'$\\\\epsilon_i \\\\sim \\\\mathcal{{N}}(0, \\\\sigma^2)$')\n",
    "    ax[1].axvline(ud, c='r', linestyle='--', label='$\\\\phi_i = {}$'.format(ud))\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].set_title('Gaussian Relaxation')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#save figure in pdf\n",
    "# fig.savefig('Bernoulli_relaxation.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
